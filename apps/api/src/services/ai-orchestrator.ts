import { OpenAI } from "openai"
import Anthropic from "@anthropic-ai/sdk"
import { AIModel, type Message } from "@prisma/client"

interface AIResponse {
  content: string
  model: AIModel
  usage?: {
    promptTokens: number
    completionTokens: number
  }
}

interface WorkflowStep {
  model: AIModel
  role: "planner" | "analyzer" | "executor"
  prompt: string
}

export class AIOrchestrator {
  private openai: OpenAI
  private anthropic: Anthropic

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    })

    this.anthropic = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
    })
  }

  async processUserRequest(userMessage: string, context: Message[], userPreferences: any): Promise<AIResponse> {
    // ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø·Ù„Ø¨ ÙˆØªÙˆØ¬ÙŠÙ‡Ù‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
    const requestType = await this.analyzeRequestType(userMessage)

    switch (requestType) {
      case "code_generation":
        return this.executeCodeWorkflow(userMessage, context, userPreferences)
      case "analysis":
        return this.executeAnalysisWorkflow(userMessage, context, userPreferences)
      case "general":
        return this.executeGeneralWorkflow(userMessage, context, userPreferences)
      default:
        return this.executeGeneralWorkflow(userMessage, context, userPreferences)
    }
  }

  private async executeCodeWorkflow(
    userMessage: string,
    context: Message[],
    userPreferences: any,
  ): Promise<AIResponse> {
    // Ø§Ù„Ø®Ø·ÙˆØ© 1: GPT-4 ÙŠØ±Ø§Ø¬Ø¹ Ø§Ù„ÙÙƒØ±Ø© ÙˆÙŠÙ†Ø´Ø¦ Ø®Ø·Ø©
    const planningStep = await this.callGPT4({
      role: "planner",
      prompt: this.buildPlanningPrompt(userMessage, userPreferences),
      context,
    })

    // Ø§Ù„Ø®Ø·ÙˆØ© 2: Claude ÙŠØ­Ù„Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ ÙˆÙŠØ¬Ù‡Ø² Ø§Ù„Ø¨Ù†ÙŠØ©
    const analysisStep = await this.callClaude({
      role: "analyzer",
      prompt: this.buildAnalysisPrompt(planningStep.content, userMessage),
      context: [...context, { role: "assistant", content: planningStep.content }],
    })

    // Ø§Ù„Ø®Ø·ÙˆØ© 3: DeepSeek ÙŠÙ†ÙØ° Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙØ¹Ù„ÙŠ
    const executionStep = await this.callDeepSeek({
      role: "executor",
      prompt: this.buildExecutionPrompt(analysisStep.content, userMessage),
      context: [
        ...context,
        { role: "assistant", content: planningStep.content },
        { role: "assistant", content: analysisStep.content },
      ],
    })

    return {
      content: this.combineWorkflowResults([planningStep, analysisStep, executionStep]),
      model: AIModel.DEEPSEEK_CODER,
      usage: {
        promptTokens:
          planningStep.usage?.promptTokens + analysisStep.usage?.promptTokens + executionStep.usage?.promptTokens,
        completionTokens:
          planningStep.usage?.completionTokens +
          analysisStep.usage?.completionTokens +
          executionStep.usage?.completionTokens,
      },
    }
  }

  private async callGPT4({ role, prompt, context }: WorkflowStep & { context: any[] }): Promise<AIResponse> {
    const messages = [
      {
        role: "system",
        content: `Ø£Ù†Øª Ù…Ø®Ø·Ø· Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø· ØªÙ†ÙÙŠØ°ÙŠØ© Ù…ÙØµÙ„Ø©. 
        Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ ÙÙ‡Ù… Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ø®Ø·Ø© Ø¹Ù…Ù„ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø¸Ù…Ø©.
        ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆÙ…ÙØµÙ„Ø© ÙˆÙ…Ù†Ø·Ù‚ÙŠØ©.`,
      },
      ...context.map((msg) => ({
        role: msg.role,
        content: msg.content,
      })),
      {
        role: "user",
        content: prompt,
      },
    ]

    const response = await this.openai.chat.completions.create({
      model: "gpt-4",
      messages: messages as any,
      temperature: 0.7,
      max_tokens: 2000,
    })

    return {
      content: response.choices[0].message.content || "",
      model: AIModel.GPT4,
      usage: {
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0,
      },
    }
  }

  private async callClaude({ role, prompt, context }: WorkflowStep & { context: any[] }): Promise<AIResponse> {
    const systemPrompt = `Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ù†Ø·Ù‚ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø·Ø· ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©.
    Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ Ø£Ø®Ø° Ø§Ù„Ø®Ø·Ø© Ù…Ù† GPT-4 ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§ Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.
    ÙŠØ¬Ø¨ Ø£Ù† ØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©.`

    const messages = context.map((msg) => ({
      role: msg.role === "assistant" ? "assistant" : "user",
      content: msg.content,
    }))

    messages.push({
      role: "user",
      content: prompt,
    })

    const response = await this.anthropic.messages.create({
      model: "claude-3-sonnet-20240229",
      system: systemPrompt,
      messages: messages as any,
      max_tokens: 2000,
      temperature: 0.5,
    })

    return {
      content: response.content[0].type === "text" ? response.content[0].text : "",
      model: AIModel.CLAUDE_SONNET,
      usage: {
        promptTokens: response.usage.input_tokens,
        completionTokens: response.usage.output_tokens,
      },
    }
  }

  private async callDeepSeek({ role, prompt, context }: WorkflowStep & { context: any[] }): Promise<AIResponse> {
    // DeepSeek API call (assuming similar to OpenAI)
    const messages = [
      {
        role: "system",
        content: `Ø£Ù†Øª Ù…Ø·ÙˆØ± Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙØ¹Ù„ÙŠ.
        Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ Ø£Ø®Ø° Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ø¨Ù†ÙŠØ© Ù…Ù† Claude ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ÙƒÙˆØ¯ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙ†ÙÙŠØ°.
        ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„ÙƒÙˆØ¯ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆÙ…ÙØ­Ø³ÙÙ‘Ù† ÙˆÙ…ÙÙˆØ«ÙÙ‘Ù‚ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.`,
      },
      ...context.map((msg) => ({
        role: msg.role,
        content: msg.content,
      })),
      {
        role: "user",
        content: prompt,
      },
    ]

    // Simulated DeepSeek API call
    const response = await fetch("https://api.deepseek.com/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${process.env.DEEPSEEK_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "deepseek-coder",
        messages,
        temperature: 0.3,
        max_tokens: 3000,
      }),
    })

    const data = await response.json()

    return {
      content: data.choices[0].message.content,
      model: AIModel.DEEPSEEK_CODER,
      usage: {
        promptTokens: data.usage.prompt_tokens,
        completionTokens: data.usage.completion_tokens,
      },
    }
  }

  private buildPlanningPrompt(userMessage: string, userPreferences: any): string {
    return `
Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ${userMessage}

ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
- Ø§Ù„Ù„ØºØ©: ${userPreferences.language || "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"}
- Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù…Ù„: ${userPreferences.workType || "Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª"}
- Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¹Ù…Ù„: ${userPreferences.workStyle || "Ù…Ù†Ù‡Ø¬ÙŠ ÙˆÙ…ÙØµÙ„"}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ:
1. ÙÙ‡Ù… Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø¯Ù‚Ø©
2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„ÙØ±Ø¹ÙŠØ©
3. Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø© ØªÙ†ÙÙŠØ°ÙŠØ© Ù…Ø±Ø­Ù„ÙŠØ©
4. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
5. ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„Ø¬Ù‡Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨

ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø®Ø·Ø© Ù…ÙØµÙ„Ø© ÙˆÙ…Ù†Ø¸Ù…Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
    `
  }

  private buildAnalysisPrompt(planContent: string, userMessage: string): string {
    return `
Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© Ù…Ù† GPT-4:
${planContent}

Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…:
${userMessage}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ ÙƒÙ…Ø­Ù„Ù„ Ù…Ù†Ø·Ù‚ÙŠ:
1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© ÙˆÙ…Ø±Ø§Ø¬Ø¹ØªÙ‡Ø§
2. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
3. ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø¶Ø¹Ù ÙÙŠ Ø§Ù„Ø®Ø·Ø©
4. Ø§Ù‚ØªØ±Ø§Ø­ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø£Ùˆ ØªØ¹Ø¯ÙŠÙ„Ø§Øª
5. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©

ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ ÙˆÙ…ÙˆØ§ØµÙØ§Øª ØªÙ‚Ù†ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø©.
    `
  }

  private buildExecutionPrompt(analysisContent: string, userMessage: string): string {
    return `
Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ù…Ù† Claude:
${analysisContent}

Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…:
${userMessage}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ ÙƒÙ…Ø·ÙˆØ± Ø®Ø¨ÙŠØ±:
1. ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙØ¹Ù„ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„
2. ÙƒØªØ§Ø¨Ø© ÙƒÙˆØ¯ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆÙ…ÙØ­Ø³ÙÙ‘Ù†
3. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙˆØ«ÙŠÙ‚ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
4. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§ØªØ¨Ø§Ø¹ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª
5. ØªÙ‚Ø¯ÙŠÙ… Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙ†ÙÙŠØ° Ù…Ø¹ Ø§Ù„Ø´Ø±Ø­.
    `
  }

  private combineWorkflowResults(steps: AIResponse[]): string {
    return `
# Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©

## ğŸ“‹ Ø§Ù„Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© (GPT-4)
${steps[0].content}

---

## ğŸ” Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ (Claude)
${steps[1].content}

---

## ğŸ’» Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠ (DeepSeek)
${steps[2].content}

---

*ØªÙ… Ø¥Ù†ØªØ§Ø¬ Ù‡Ø°Ù‡ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ¹Ø§ÙˆÙ† Ø«Ù„Ø§Ø«Ø© Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒÙŠØ© Ù…ØªØ®ØµØµØ© Ù„Ø¶Ù…Ø§Ù† Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø© ÙˆØ¯Ù‚Ø©.*
    `
  }

  private async analyzeRequestType(message: string): Promise<string> {
    const codeKeywords = ["ÙƒÙˆØ¯", "Ø¨Ø±Ù…Ø¬Ø©", "ØªØ·Ø¨ÙŠÙ‚", "Ù…ÙˆÙ‚Ø¹", "API", "database", "function"]
    const analysisKeywords = ["ØªØ­Ù„ÙŠÙ„", "Ø¯Ø±Ø§Ø³Ø©", "Ù…Ù‚Ø§Ø±Ù†Ø©", "ØªÙ‚ÙŠÙŠÙ…", "Ø¨Ø­Ø«"]

    const lowerMessage = message.toLowerCase()

    if (codeKeywords.some((keyword) => lowerMessage.includes(keyword))) {
      return "code_generation"
    }

    if (analysisKeywords.some((keyword) => lowerMessage.includes(keyword))) {
      return "analysis"
    }

    return "general"
  }
}
